<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta charset="utf-8" />
  <meta content="width=device-width, initial-scale=1" name="viewport" />
  <title>
    Noise-Aware Generative Microscopic Traffic Simulation
  </title>
  <meta content="RFC" property="og:title" />
  <meta content="Accurately modeling individual vehicle behavior in microscopic traffic simulation remains a key challenge in intelligent transportation systems, as it requires vehicles to realistically generate and respond to complex traffic phenomena such as phantom traffic jams. While traditional human driver simulation models like the Intelligent Driver Model offer computational tractability, they do so by abstracting away the very complexity that defines human driving. On the other hand, recent advances in infrastructure-mounted camera-based roadway sensing have enabled the extraction of vehicle trajectory data, presenting an opportunity to shift toward generative, agent-based models that learn to reproduce driving behaviors directly from data. Yet, a major bottleneck remains: most existing datasets are either overly sanitized or lack standardization, failing to reflect the noisy, imperfect nature of real-world sensing. Unlike data from vehicle-mounted sensors—which can mitigate sensing artifacts like occlusion through overlapping fields of view and sensor fusion— infrastructure-based sensors surface a messier, more practical view of challenges that traffic engineers face every day. To this end, we present the I-24 MOTION Scenario Dataset (I24-MSD)—a standardized, curated dataset designed to preserve a realistic level of sensor imperfection, embracing these errors as part of the learning problem rather than an obstacle to overcome purely from preprocessing. Drawing from noise-aware learning strategies in computer vision, we further adapt existing generative models in the autonomous driving community for I24-MSD with noise-aware loss functions. Our results show that such models outperform traditional baselines in terms of simulation realism." name="description" property="og:description" />
  <meta content="https://ct135.github.io/i24-msd/" property="og:url" />
  <meta name="keywords" content="Intelligent Transportation Systems; Imitation Learning; I24-MSD">

  <link rel="stylesheet" href="assets/css/project_stylesheet.css">
  <link href="data/misc/favicon.ico" rel="shortcut icon">
  <link href="data/misc/favicon_apple.ico" rel="apple-touch-icon">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="assets/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="assets/academicons/css/academicons.min.css">

  <script defer src="assets/js/fontawesome.all.min.js"></script>
</head>

<body>
  <div class="n-header">
  </div>
  <div class="n-title">
    <h1>
      Noise-Aware Generative Microscopic Traffic Simulation
    </h1>
  </div>
  <div class="n-byline">
    <div class="byline">
      <ul class="authors">
        <li>
          <a href="https://vindulamj.github.io" target="_blank">Vindula Jayawardana</a><sup>1</sup>
        </li>
        <li>
          <a href="https://www.linkedin.com/in/catherine-tang-76699a193" target="_blank">Catherine Tang</a><sup>1</sup>
        </li>
        <li>
          <a href="https://research.nvidia.com/person/pavlo-molchanov/" target="_blank">Junyi Ji</a><sup>2</sup>
        </li>
        <li>
          <a href="http://www.cs.cmu.edu/~kkitani/" target="_blank">Jonah Philion</a><sup>3</sup>
        </li>
        <li>
          <a href="https://jankautz.com/" target="_blank">Xue Bin Peng</a><sup>3</sup>
        </li>
        <li>
          <a href="http://www.wucathy.com/blog/" target="_blank">Cathy Wu</a><sup>1</sup>
        </li>
      </ul>
      <ul class="authors affiliations">
        <li>
          <sup>
            1
          </sup>
          MIT
        </li>
        <li>
          <sup>
            2
          </sup>
          Vanderbilt
        </li>
        <li>
          <sup>
            3
          </sup>
          NVIDIA
        </li>
      </ul>
      <ul class="authors venue">
        <li>
          CVPR 2022 (Oral)
        </li>
      </ul>
      <ul class="authors links">
        <li>
          <a href="https://arxiv.org/pdf/2112.01524.pdf" target="_blank">
            <button class="btn"><i class="fa fa-file-pdf"></i> Paper</button>
          </a>
        </li>
        <li>
          <a href="https://youtu.be/wpObDXcYueo" target="_blank">
            <button class="btn"><i class="fab fa-youtube"></i> Video</button>
          </a>
        </li>
        <li>
          <a href="https://github.com/NVlabs/GLAMR" target="_blank">
            <button class="btn"><i class="fab fa-github"></i> Code</button>
          </a>
        </li>
      </ul>
    </div>
  </div>

  <div class="n-article">
    <div class="n-page video">
      <video class="centered shadow" width="100%" autoplay muted loop playsinline>
        <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
        <source src="assets/media/glamr_teaser.mp4#t=0.001" type="video/mp4" />
      </video>
      <div class="videocaption" style="margin-bottom: 1rem">
        <div>
          GLAMR (Left) recovers human meshes in consistent <strong>global</strong> coordinates from videos captured by <strong>dynamic cameras</strong> and <strong>infills</strong> missing poses (transparent) due to various
          occlusions (obstruction, missed detection, outside field of view), while standard human mesh recovery methods (Right) fail to do so.
        </div>   
      </div>
    </div>

    <h2 id="abstract">
      Abstract
    </h2>
    <p>
      Accurately modeling individual vehicle behavior in microscopic traffic simulation remains a key challenge in intelligent transportation systems, as it requires vehicles to realistically generate and respond to complex traffic phenomena such as phantom traffic jams. While traditional human driver simulation models like the Intelligent Driver Model offer computational tractability, they do so by abstracting away the very complexity that defines human driving. On the other hand, recent advances in infrastructure-mounted camera-based roadway sensing have enabled the extraction of vehicle trajectory data, presenting an opportunity to shift toward generative, agent-based models that learn to reproduce driving behaviors directly from data. Yet, a major bottleneck remains: most existing datasets are either overly sanitized or lack standardization, failing to reflect the noisy, imperfect nature of real-world sensing. Unlike data from vehicle-mounted sensors—which can mitigate sensing artifacts like occlusion through overlapping fields of view and sensor fusion— infrastructure-based sensors surface a messier, more practical view of challenges that traffic engineers face every day. To this end, we present the I-24 MOTION Scenario Dataset (I24-MSD)—a standardized, curated dataset designed to preserve a realistic level of sensor imperfection, embracing these errors as part of the learning problem rather than an obstacle to overcome purely from preprocessing. Drawing from noise-aware learning strategies in computer vision, we further adapt existing generative models in the autonomous driving community for I24-MSD with noise-aware loss functions. Our results show that such models outperform traditional baselines in terms of simulation realism.
    </p>

    <h2 id="results">
      Results
    </h2>
    <h3 class="results" id="sample">
      Generative Motion Infilling with Multiple Samples
    </h3>
    <video class="centered shadow" width="100%" autoplay muted loop playsinline>
      <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
      <source src="assets/media/glamr_sample.mp4#t=0.001" type="video/mp4" />
    </video>
    <div class="videocaption">
      <div>GLAMR uses generative motion infiller to infill multiple plausible motions for invisible people.</div>
    </div>

    <h3 class="results">
      3DPW Sequences
    </h3>
    <video class="centered shadow video_without_cap" width="100%" autoplay muted loop playsinline>
      <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
      <source src="assets/media/glamr_res1.mp4#t=0.001" type="video/mp4" />
    </video>
    <video class="centered shadow video_without_cap" width="100%" autoplay muted loop playsinline>
      <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
      <source src="assets/media/glamr_res2.mp4#t=0.001" type="video/mp4" />
    </video>
    <video class="centered shadow video_without_cap" width="100%" autoplay muted loop playsinline>
      <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
      <source src="assets/media/glamr_res3.mp4#t=0.001" type="video/mp4" />
    </video>
    
    
    <h2>
      Narrated Results Video
    </h2>
    <div class="video_wrapper shadow">
      <iframe width="705" height="397" border-style=none src="https://www.youtube.com/embed/wpObDXcYueo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </div>

    <h2>
      Overview
    </h2>
    <img class="figure" src="assets/media/glamr_overview.png" alt="GLAMR Overview">

    <h2 id="citation">
      Citation
    </h2>
    <pre class="bibtex">
      <code>
@inproceedings{yuan2022glamr,
  title={GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras},
  author={Yuan, Ye and Iqbal, Umar and Molchanov, Pavlo and Kitani, Kris and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}
      </code></pre>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
              <a href="https://nvlabs.github.io/GLAMR" target="_blank" style="font-size: small;">Template</a>
            </p>
          </td>
        </tr>
      </tbody></table>

  </div>
</body>

</html>
